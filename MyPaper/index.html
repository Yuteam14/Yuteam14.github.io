<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CLIP相关论文 | MyPaper</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .container-lg {
            max-width: 1012px;
            margin-right: auto;
            margin-left: auto;
        }
        .markdown-body {
            font-family: -apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif;
            font-size: 16px;
            line-height: 1.5;
        }
        .paper {
            margin-bottom: 20px;
            padding: 15px;
            background: #f9f9f9;
            border-left: 4px solid #3498db;
        }
        a {
            color: #0366d6;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        blockquote {
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
            margin: 0 0 16px 0;
        }
        .notes-link {
            display: block;
            text-align: center;
            margin-top: 30px;
            font-size: 14px;
            position: relative;
        }
        .notes-dropdown {
            display: none;
            position: absolute;
            background-color: white;
            min-width: 200px;
            box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
            z-index: 1;
            left: 50%;
            transform: translateX(-50%);
            border-radius: 4px;
            padding: 10px 0;
        }
        .notes-dropdown a {
            color: black;
            padding: 8px 16px;
            display: block;
            text-align: center;
        }
        .notes-dropdown a:hover {
            background-color: #f1f1f1;
        }
        .notes-link:hover .notes-dropdown {
            display: block;
        }
    </style>
</head>
<body>
    <div class="container-lg px-3 my-5 markdown-body">
        <h1><a href="https://yuteam14.github.io/">返回主页</a> | <a href="https://yuteam14.github.io/MyPaper/">MyPaper</a></h1>
        
        <h3 id="视觉语言模型增强">视觉语言模型增强：</h3>
        <ol>
            <!-- 论文1  -->
            <li>
                [2024-arXiv] <strong>Boosting Vision-Language Models with Transduction</strong>
                <a href="https://arxiv.org/pdf/2406.01837" target="_blank">[paper]</a>
                <a href="https://github.com/MaxZanella/transduction-for-vlms">[code]</a>
                <a href="Note/文献学习笔记-2.pdf">[笔记]</a>
                <blockquote>
                    <p>摘要提出基于统计适应的转导推理方法，在11个跨模态基准测试中平均提升8.7%准确率</p>
                </blockquote>
            </li>
            
            <!-- 论文2 -->
            <li>
                [2024-arXiv] <strong>Enhancing Remote Sensing Vision-Language Models for Zero-Shot Scene Classification</strong>
                <a href="https://arxiv.org/pdf/2409.00698" target="_blank">[paper]</a>
                <a href="https://github.com/elkhouryk/RS-TransCLIP">[code]</a>
                <a href="Note/文献学习笔记-2.pdf">[笔记]</a>
                <blockquote>
                    <p>针对遥感图像的零样本场景分类任务，提出多尺度特征对齐方法</p>
                </blockquote>
            </li>
            
            <!-- 论文3 -->
            <li>
                [2024-arXiv] <strong>Boosting Vision-Language Models for Histopathology Classification: Predict All at Once</strong>
                <a href="https://arxiv.org/pdf/2409.01883" target="_blank">[paper]</a>
                <a href="https://github.com/FereshteShakeri/Histo-TransCLIP">[code]</a>
                <blockquote>
                </blockquote>
            </li>

            <!-- 论文4 -->
            <li>
                [2025-CVPR] <strong>SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images: Predict All at Once</strong>
                <a href="https://arxiv.org/pdf/2410.01768" target="_blank">[paper]</a>
                <blockquote>
                </blockquote>
            </li>

            <!-- 论文5 -->
            <li>
                [2025-CVPR] <strong>XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?: Predict All at Once</strong>
                <a href="https://arxiv.org/pdf/2503.23771" target="_blank">[paper]</a>
                <blockquote>
                </blockquote>
            </li>
            
        </ol>
        
        <div class="notes-link">
            <a href="#">浏览笔记 ▼</a>
            <div class="notes-dropdown">
                <a href="Note/文献学习笔记-1.pdf" target="_blank">文献阅读笔记-1</a>
                <a href="Note/文献学习笔记-2.pdf" target="_blank">文献阅读笔记-1</a>
                <!-- 未来可以在这里添加更多笔记链接 -->
                <!-- <a href="Note/文献阅读笔记-2.pdf" target="_blank">文献阅读笔记-2</a> -->
            </div>
        </div>
    </div>
</body>
</html>
